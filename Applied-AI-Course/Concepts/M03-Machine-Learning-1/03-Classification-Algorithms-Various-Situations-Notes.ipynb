{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9968f18",
   "metadata": {},
   "source": [
    "__Introduction__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162614438-98c6cd10-39d0-4b0a-89e5-7e2629b262c8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f1c84",
   "metadata": {},
   "source": [
    "__Imbalanced vs Balanced Dataset__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162614632-c3ab4cb0-e3e9-465e-89e2-4d459febdeeb.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162614718-11e74272-d450-4b1e-8bed-69773285dd7f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162614816-82d89532-2b33-4b62-aa0f-9ef21baeb695.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162614854-1b9b7ca4-a3be-4e55-ac49-2f3f8dbf6bee.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162614905-a0bb7425-a9ec-4fb0-99b2-890946a8db04.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162614965-7529f93f-fe39-4360-8ab6-2b516401825e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162615026-a9e7f0d4-355f-467a-9943-0d24524b45f6.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162615099-9705dd95-40ac-4f93-9dcc-fc7e313f188a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162615172-2a411fcf-3212-4c35-9602-52626791a138.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162615220-de78e3f8-a655-438f-96e0-929a2c833f25.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162615314-0e48bc04-ede1-420c-aa34-e1edd98477c1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38257a0",
   "metadata": {},
   "source": [
    "__Multi-Class Classification__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162615873-38cec527-ae6b-4b7c-a8d5-198822d2f580.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162615966-81ff6464-61d2-4839-87f0-cd8023415720.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162616030-592c680b-bc97-4820-b668-5e7afe2cae2c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162616143-48f2d00c-8d4b-46e9-85e9-48b816996cc6.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162616256-7434b0ee-4f60-4854-808b-dd5c6f407127.png)\n",
    "\n",
    "In One vs Rest method, time and space complexity can increase.\n",
    "\n",
    "One vs rest is defined like that. There is no solution for it. You need to feed the query point to n models and get the probabilities. Then, choose the model for which the probability is high. Then, return the class corresponding to that model. As the number of classes increases, there would be a increase in time complexity too. It's a trade-off.\n",
    "\n",
    "Given a query point $x_q$ we will send it to each model and they will return me the probability of point belong to each class and whichever class have highest probability then $x_q$ will belong to that class.\n",
    "\n",
    "If probability of $x_q$ belonging to class B is 0.6 and probability of $x_q$ belonging to class C is also 0.6 then we can randomly pick B or C and assign $x_q$ to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d298d36",
   "metadata": {},
   "source": [
    "__K-NN, given a Distance or Similarity Matrix__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162627309-86425bda-96bc-40b8-b78e-f537964f6484.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162627486-b0e8dd3f-1fc6-4903-b9af-dded9876b279.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162627698-816fbe29-5957-4271-bca7-a9c237a56661.png)\n",
    "\n",
    "Logistic Regression needs vectors explicitly.\n",
    "\n",
    "Here without similarities data we cannot proceed further in evaluating our model. Model will help us find the nearest neighbours given the similarity data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718cd329",
   "metadata": {},
   "source": [
    "__Train and Test set differences__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162628222-e11899a0-9e42-487f-be7a-5b00d3782adc.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162628495-737b2fb7-e532-4dfe-974e-d61bf9069d3e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162628591-770207b5-3183-4d5f-8e95-92b4cac46bbe.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162628659-e6daa550-da2a-4d10-bbad-8472e1344f0a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162628816-26917719-30ed-46d4-a357-fc5e8b3e9887.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162628980-0745806e-89a8-49a1-ba5d-8a0f973f8cf9.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162629069-64126f33-a483-426b-bd11-3084f49e70de.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162629254-c86b070c-74bf-445f-ad99-bf993aad8e36.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/162629308-65790f53-419e-4cf9-b884-4818f50f65ef.png)\n",
    "\n",
    "In a nutshell, we change $D_n$ to $D_n^{'}$, by mixing Dtrain and Dtest, such that Dtrain is represented by a class label 1 and Dtest is represented by a class label 0. We then ask the classifier to separate the points in two classes. If they get well separated, they are highly dissimilar.\n",
    "\n",
    "If distributions of train and test gets changed over time then there will not be a perfect overlap of +ve and -ve points of train and test data. There might be some overlap. In case of classifier we want them to more or less perfectly overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc22706",
   "metadata": {},
   "source": [
    "__Impact of Outliers__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163665083-d95af77b-3535-4d8f-9a44-a76a6f058a94.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163665188-e61bd930-9a59-43e1-8f49-d0fae2090875.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163665249-427bef4c-3aab-4a8d-838b-a6b42cc62fc5.png)\n",
    "\n",
    "Noise may appear randomly in a dataset, but outliers are the once which are significantly different from the remaining dataset. An outlier is not a false value or void in meaning. It is definite and accurate but when it is linked with the other tuples in your model, it is just not in the same range Where as, Noise is garbage. Void, Null information that is not useful at all, under any circumstances.\n",
    "For example: Outlier could be the unusual identifiable patterns of data seen in MRI scans that help detect the symptoms of disease. Noise on the other hand may not be of interest for analysis.\n",
    "\n",
    "As we know by definition outliers are points that are distant from remaining observations. As a result, they can potentially skew or bias any analysis performed on the dataset. It is therefore very important to detect and adequately deal with outliers.\n",
    "\n",
    "But Outliers are not always bad data points. Sometimes they give important information of all for example in case of MRI scan.\n",
    "\n",
    "So when dealing with outliers they should only be discarded when you are 100% sure they were a result of an experimental/transcription/etc error. Otherwise removing outliers may result in underestimated variance. So whenever you find an outlier you should analyze it in context. Is it a bad data point? Is it a falsified measurement? Or is it correct but too extreme for the model you want to achieve? and Justify your decision to drop/impute/keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244212f",
   "metadata": {},
   "source": [
    "__Local Outlier Factor (Simple solution: Mean distance to Knn)__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163665652-20bd4814-8466-4a92-8f45-c324ff05fc4b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163665710-4d47ce54-c948-48a1-92c8-3a9dd59f0b04.png)\n",
    "\n",
    "Problem with above approach is, local density is not captured.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163665738-2a5a5602-a229-44e1-b824-4ab3aed4a46e.png)\n",
    "\n",
    "Finding the right 'K' is very important if you choose to use this simple approach. One data-driven way to find the appropriate K is to use the elbow/knee method described many times in this course for hyper-param tuning as follows:\n",
    "1. Obtain the number of outliers for various values of k.\n",
    "2. Plot \"k\" on x-axis and number of outliers on the y-axis.\n",
    "3. In this plot, you will notice a sudden change in the curve shape at some \"k\". This point is called the elbow/knee point or inflection point.\n",
    "4. Pick the \"k\" corresponding to the elbow point as the appropriate \"k\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b2fbb",
   "metadata": {},
   "source": [
    "__K Distance__ - it is the farthest distance from $x_i$ to a point in the neighborhood set.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163666253-330e7410-69f6-40b5-9f5d-aaa89ab274a2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f046f",
   "metadata": {},
   "source": [
    "__Reachability-Distance(A, B)__ answers this question: Within what maximum distance a point is reachable?\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163666692-c67372d4-3cb3-4143-8d69-1997e097a90e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b88a0",
   "metadata": {},
   "source": [
    "__Local Reachability Density(A)__ (lrd)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163667069-c4b062cd-23a3-418c-b4d8-6f6199fc0e43.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163667292-66eae6d9-6bbc-44e0-b602-d18728b56b9b.png)\n",
    "\n",
    "Link: https://youtu.be/GJAOpTSh0xA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e5fa8",
   "metadata": {},
   "source": [
    "__Local Outlier Factor(A)__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163670992-8c25ceee-a4b2-42e2-a966-957467181da0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163671059-1bd87c36-0885-4624-ab38-8d0973ffb1ba.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163671274-54972236-fd70-4f2f-a5b2-97bc51c073d7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163671337-0499d64c-2e21-4beb-be64-4e7afbdb83c8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163671494-c4d08d3a-439f-4a9e-a2d1-de820bcefb0e.png)\n",
    "\n",
    "Box-plots are for univariate outlier detection while LOF are for multivariate outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6349c93",
   "metadata": {},
   "source": [
    "__Impact of Scale & Column Standardization__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163671862-6f204de9-cf00-41a1-bbb1-80b19b9108fe.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163671862-6f204de9-cf00-41a1-bbb1-80b19b9108fe.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163672004-90ef38af-9127-4acb-86df-6b295d441a82.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c73fa4",
   "metadata": {},
   "source": [
    "__Interpretability__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163672577-b4cab6a5-e274-4e38-a5d8-3c0ad0406a89.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163672664-abae85cf-9e2f-4522-bd88-c5432bbda921.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163672763-c591b133-fc7e-4faa-bd20-b7eef1a74a77.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163672821-272c5ccc-50b6-45d7-bba3-816fc27652ec.png)\n",
    "\n",
    "The more a machine’s decision affects a person’s life, the more important it is for the machine to explain its behavior. Interpretability is a useful debugging tool for detecting bias in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc492a",
   "metadata": {},
   "source": [
    "__Feature Importance and Forward Feature Selection__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673410-41bede0e-901f-4385-bcf7-a71ab75cab3b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673456-989a07b0-0eb8-40af-a840-5f24fd85fbb1.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673527-1f5c5482-208b-4ef6-8b74-66861fa9e031.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673640-29d20b81-9de3-438f-8059-dcc1182db4a4.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673753-184e13d6-6280-4dc7-ab47-86ab057cdf57.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673804-7740937b-a90d-487d-b896-771aa66bab82.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673922-eeeb010b-dad6-4e1a-82ed-de974e32fde8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163673968-d2028a35-53a9-475a-aec5-7bbfa9885ea9.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163674008-6545021d-0eb7-44f8-ba96-2f3f0e334f9d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163674077-751ff144-a1f5-429e-8803-eab9d77fda6e.png)\n",
    "\n",
    "PCA is used exclusively to reduce the number of dimensions but not from the existing features. It creates all new features and if we want to reduce the dimensionality from a to b where (b < a) then among all the a features given as an input to the dataset, those top b features are selected which could preserve maximum variance of the data. We could not use PCA of Forward Feature Selection/Backward Elimination as PCA belongs to the category of Feature Engineering Techniques and Forward Feature Selection/Backward Elimination belong to the category of Feature Selection Techniques.\n",
    "\n",
    "In real world there can be cases when we have very high dimensional data and following that approach will be very time consuming but in some cases with very low dimension we can follow this approach and the result will certainly be better.\n",
    "\n",
    "Link: https://youtu.be/rqMR4oTT85k\n",
    "\n",
    "Lets make a note of difference between PCA and Forward or Backward selection.\n",
    "1. Forward or Backward selection comes under feature selection techniques where you pick some subset of features from the original set.\n",
    "2. PCA comes under a dimensionality reduction technique where it creates some complex features from the existing set of features and then discards the less important ones.\n",
    "3. If the data is not linearly separable, then you have to try applying transforms to the existing features and check if you could separate the classes. If it works, then that would be fine. Otherwise, you need to go with creating a completely new set of features that could preserve either the maximum variance or the neighborhood, in this case we use PCA because PCA creates complex set of features from existing ones.\n",
    "4. In case, if you are asked not to create a new set of features or if you find the data already to be linearly separable and are looking to get rid of the curse of dimensionality, then you have to go for Feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0e25f",
   "metadata": {},
   "source": [
    "__Handling Categorical and Numerical Features__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163706800-cc69f746-182b-4b15-98e7-f7109dc14060.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707040-aa0a2d6e-deaf-4a01-852f-bb744d976443.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163706995-22872061-9d42-40b8-aac6-9ecb940bdc99.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707159-67cf01fa-ebd3-46c7-955d-2e232ae2802f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707231-88da0786-0e5a-4f40-8c93-ed15b42ee476.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707319-2b6da3bb-2ae2-4241-bd34-fcda3a4905e6.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707419-e15c581a-0b15-445b-9bc6-063b0676f6d9.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707513-d9b31b93-0d44-4023-812e-3883f5393a2b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707592-dbbd90bb-e6c2-4532-a779-1c2da42cfc69.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163707645-13abd2cc-b388-477d-94a5-7326d81a56e6.png)\n",
    "\n",
    "One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality.\n",
    "\n",
    "Though label encoding is straight but it has the disadvantage that the numeric values can be misinterpreted by algorithms as having some sort of hierarchy/order in them. There is a very high probability that the model captures the relationship between countries such as India < Japan < US which is undesirable.\n",
    "\n",
    "One hot encoding has to be applied if there are any categorical features in our data and if we want to convert them into numerical format. Applying PCA on the new features obtined through one-hot encoding, could not yield much variance as these features contain only 1's and 0's. So if we apply PCA on these one hot encoded features, these features will definitely be dropped as they couldn't preserve much variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa0186",
   "metadata": {},
   "source": [
    "__Handling Missing Values by Imputation__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163708182-f219f3d7-58c4-4b08-ab90-ceaef33cdf17.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163708273-0b8d96c3-e47d-42e4-a1f5-fe2752804ff0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163708361-1e58ef78-9c30-424a-803d-c551ce42a217.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163708573-16c90829-fcf9-45b0-b4e4-3699fbcfbd2f.png)\n",
    "\n",
    "This is one of the method to handle missing value if we have latency requirement then we can drop the 3rd idea.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163708664-01a77d69-bcc7-4f5c-bec0-b397dd12e85f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163708734-bed19dcb-070c-46bb-b887-0646949076b5.png)\n",
    "\n",
    "Model based imputers take significantly more time to run compated to simple, mean, median and mode based imputers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39413b",
   "metadata": {},
   "source": [
    "__Curse of Dimensionality__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163712224-41dc3312-be67-4c67-8266-54be958d9864.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163712285-91b83a5f-231f-4e90-bcc8-a392b71b2874.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163712504-aa99db12-4c6b-4375-9eb5-0312c2a0e065.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163712611-0aca3af6-9dbb-471c-9b4e-69bf4a58d578.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163712707-6b369988-aec6-4136-980a-db1a6231cdbd.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163712803-dab5ac69-d961-4ee0-a240-7a5502327e20.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163712944-c1d7423b-1c39-4308-880e-d6020ee7eef2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163713077-6727a7d2-f00e-4a20-b439-b6a1bfb3d516.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163713175-d8a0d067-dec3-489d-8d9e-6c937a3dbf62.png)\n",
    "\n",
    "Let's say you have a straight line 100 yards long and you dropped a penny somewhere on it. It wouldn't be too hard to find. You walk along the line and it takes two minutes.\n",
    "\n",
    "Now let's say you have a square 100 yards on each side and you dropped a penny somewhere on it. It would be pretty hard, like searching across two football fields stuck together. It could take days.\n",
    "\n",
    "Now a cube 100 yards across. That's like searching a 30-story building the size of a football stadium. Ugh.\n",
    "\n",
    "The difficulty of searching through the space gets a *lot* harder as you have more dimensions. You might not realize this intuitively when it's just stated in mathematical formulas, since they all have the same \"width\". That's the curse of dimensionality. It gets to have a name because it is unintuitive, useful, and yet simple.\n",
    "\n",
    "Credits: Quora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99249ba",
   "metadata": {},
   "source": [
    "__Bias-Variance Tradeoff__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163713736-13b5535a-7608-4342-a72f-77ea6e722016.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163713808-949bc283-4760-4bd8-851e-b3f962b8b911.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163713891-134cfcb9-c2dd-4c54-861f-6292ab2857f0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163713981-fce6d0b3-4e04-4e3c-8d04-3c49c39ad739.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163714033-284548ff-a73e-40a8-9464-1ee55b51367f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163714187-48d30a90-a626-4c6a-9760-60470aa5f78a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163714246-dd34a649-c43d-474b-8b4d-21e98564d7ed.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163714330-2bb9bc87-1ca6-457a-a432-d3990109c186.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163714386-d9a0ed45-dc80-4825-bc82-2a9ea189756f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163714423-14240789-fe7a-42f9-93d6-17464a817650.png)\n",
    "\n",
    "Link: https://youtu.be/EuBBz3bI-aA\n",
    "\n",
    "We actually do not compute the values of bias and variance explicitly. We just check whether the model has more bias or variance on the basis of train and cross validation errors through the process of Cross Validation.\n",
    "\n",
    "If train error is low, but CV error is high, then the model overfits.\n",
    "\n",
    "If both the train and CV errors are high, then the model underfits.\n",
    "\n",
    "If CV error is low, then the model fits well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151c365",
   "metadata": {},
   "source": [
    "__Best and Worst Case of Algorithm__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163715280-0fd702a4-d173-4b10-b645-1ec0e09072ab.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163715347-cae6d5b4-137e-411c-941c-beb27667223d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/163715431-a0a92560-69cc-4002-a6b4-eb9f4af337fb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164c863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5377900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b271d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
