{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c82250",
   "metadata": {},
   "source": [
    "__Conditional Probability__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165064256-6c7d1ec2-860b-4198-b039-aaa668ad3e67.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165065919-abc25f8e-2361-400d-ac31-6c6cc3af96d7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165066523-b1d93a5c-5029-4fa4-ae9e-bdebf9fc616c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165066893-2383eb78-4d36-46c9-a02a-f27b991ef6d5.png)\n",
    "\n",
    "A set of two or more random variables is called uncorrelated if each pair of them are uncorrelated and there is no linear relationship between them. If X and Y are independent, with finite second moments, then they are uncorrelated. However, not all uncorrelated variables are independent.\n",
    "\n",
    "Link: https://youtu.be/JGeTcRfKgBo\n",
    "\n",
    "Link: https://www.themathcitadel.com/uncorrelated-and-independent-related-but-not-equivalent/\n",
    "\n",
    "1. Independence implies uncorrelated: which means if two random varibles are independent then they are definitely uncorrelated.\n",
    "2. Uncorreplated doesn't imply independence: which means if two random variables are uncorrelated then they 'need not be' independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad0a21",
   "metadata": {},
   "source": [
    "__Independent vs Mutually Exclusive Events__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165073473-a9358b36-cfa8-405a-88c5-777f29ad60e3.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165073938-89494490-4936-46a1-8d0b-25f2560b98d8.png)\n",
    "\n",
    "Events that does not occur at the same time are mutually exclusive events. ex: tossing a coin, heads and tails are mutually exclusive events. Either heads occur or tail occurs while tossing a coin, both does not occur at a same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eff13",
   "metadata": {},
   "source": [
    "__Bayes Theorem with examples__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165118644-d4dbe38d-655d-4a69-b12b-ddc06402337d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165119139-01ff663f-db50-46f0-aead-dfe1cf0c2e78.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165119685-3077f635-93e0-4b8a-8168-b638bf98a87a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165119995-3bd035ee-e743-4464-a221-d84adcc74f55.png)\n",
    "\n",
    "Bayes theorem is also called reverse probability, where we are usually given an event and we have to calculate the probability where it originated from.\n",
    "\n",
    "Link: https://brilliant.org/wiki/bayes-theorem/\n",
    "\n",
    "Link: https://youtu.be/YBvilAYd5sE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581ed3d",
   "metadata": {},
   "source": [
    "__Naive Bayes Algorithm__\n",
    "\n",
    "Link: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "Link: https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165335948-072f9afc-546c-40fc-b9b1-ddd69b112f4e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165336268-7795575a-c5c5-412a-8cb0-f5168ac44d55.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165336461-c19637b9-579b-4bc3-92c7-153282a8228b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4069059",
   "metadata": {},
   "source": [
    "__Naive Bayes on Text Data__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165340918-474fba13-98e2-41c7-8194-4f1e7fd4c0c7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165341501-f4040062-35c9-4894-851a-c59883a7cd51.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165341963-f56fd902-0cb7-45c0-845e-9825f43db8c4.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165342461-1a0b09e9-6320-4321-b47d-22cf7b7e5f34.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165343091-bcad95f6-913f-44a2-88b3-b9b9bbd546d1.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165343621-0ffd7dbe-5a67-44c8-88ff-51e53f12d067.png)\n",
    "\n",
    "The main goal of an ML model is to learn from the known data and make predictions on the unseen data. Hence the model should not see the test data during the training phase. The model is supposed to fit only on the train data and the features in the data also should be built only from the training data. We should not use the test data at all during the vectorization process. If you use test data also in vectorization, then the dimensionality not only increases, but also the values for those features present in the test data, but not in the train data will become 0 for all the training data points, whereas for test data points these values will be non zero. This is called Data Leakage and this should be avoided in ML model building. Hence we ignore the new terms in the test data and in place of them, we apply Laplace Smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbdd67a",
   "metadata": {},
   "source": [
    "__Laplace/Additive Smoothing__ - smoothing towards uniform distribution.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165348253-dad16581-195a-4a1e-8c81-d6d8f595fc8c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165348703-64f6f7e0-a9a8-4c6e-bd7f-53af1742c042.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165349106-47bbeeaf-92bc-44f2-8355-6d0868779a57.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165349469-3ec9c883-cda3-4d5f-929a-8c6ef5538071.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165350007-58af4047-8034-41f3-bcae-d05b8e1d0465.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165350658-c3970305-7feb-425c-92c7-d1aef7bb983c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165351263-62f01db8-b5d0-471d-893b-f02e352cbec9.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165351581-848ea45f-5a59-4afd-8786-f0b660a52e76.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165352315-efe80eea-9ffc-4aab-bdcf-9f42eeabfb42.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165353125-6ffcdb47-b1f5-47dd-aba1-692e022c0d0f.png)\n",
    "\n",
    "When I get a sentences ['abc def lmn pqr', 'abc def xyz pqr'], for training\n",
    "\n",
    "- then we store these in form of vector\n",
    "\n",
    "[\n",
    "\n",
    "['abc','def','lmn','pqr',xyz'], <= header\n",
    "\n",
    "[1,1,1,1,0], <= row1\n",
    "\n",
    "[1,1,0,1,1], <= row2\n",
    "\n",
    "]\n",
    "\n",
    "- we can even use TFIDF here if we want to add some weight\n",
    "\n",
    "- we calculate the frequencies and conditional probabilities for each class, and each feature given a class, during training phase\n",
    "\n",
    "- when new a sentence ['abc','def','ijk','xyz'] is given for testing\n",
    "\n",
    "- we conver it to\n",
    "\n",
    "[\n",
    "\n",
    "['abc','def','lmn','pqr',xyz'], <= header\n",
    "\n",
    "[1,1,1,1,0], <= row1\n",
    "\n",
    "[1,1,0,1,1], <= row2\n",
    "\n",
    "[1,1,0,0,1], <= test row\n",
    "\n",
    "]\n",
    "\n",
    "- Note no new column is added in the header, which was constructed during training\n",
    "\n",
    "- when computing the probalibites for new sentence, whenever we do not find a word, Laplace smoothing will give a non zero probability value for such word\n",
    "\n",
    "- whichever class has larger output will be considered the class for the sentence\n",
    "\n",
    "For test row, dimension will same as training and no more column will be added for new word so we are just ignoring that word while feeding query vector to model.\n",
    "\n",
    "So model will not calculate any probabilty for that word because we haven't fed extra column or anythig to model for that word.\n",
    "\n",
    "One hack to solve this problem can be add a word `unknown` to the train set manually and then call fit vectorizer on that data set. Then we can map all of the words of test that does not belong to train dictionary to `unknown` so that this problem can be solved and we can use laplace smoothing.\n",
    "\n",
    "And remember one more thing: Laplace smoothing does not only solve the problem of words or features that did not appear, it is also used for solving the problem of bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1003c54",
   "metadata": {},
   "source": [
    "__Log-Probabilities for Numerical Stability__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165792690-b7bf2edd-e973-46b7-8f28-498e80f76fac.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165793346-e065bb7c-be65-4ffb-8b8a-3ca90e01714f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165794091-123072ec-6992-48bb-a914-a2a1cc2e6759.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165794849-8f31d579-b5cc-469b-ab6b-533a8ffe64e5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda42f53",
   "metadata": {},
   "source": [
    "__Bias and Variance Tradeoff__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165801362-8cb9a882-952a-4560-8f2a-eb5d5dd5e761.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165801606-c9b62075-9b58-4534-b730-47dbd9ac7b72.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165802017-892e9643-54e1-4f64-86a0-254f659ebec1.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165802454-826638c1-ba4f-42b9-952d-a50dd0a7e55e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165803156-335fa8b8-3bab-4733-bb4c-35c184617183.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165803752-5deec089-24dd-4a1b-928c-a108139ae6b4.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165804706-0b83a1c7-6b24-4f42-b6ad-86912249329d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165804002-38f3d593-56a7-447e-b81d-94065e7ff2bf.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165804239-2f94c7c1-452b-4b42-89e5-79d223806cd8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165804487-ba4700b5-f011-4918-a97e-687a0bc4dfe0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6c1bc",
   "metadata": {},
   "source": [
    "__Feature Importance and Interpretability__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165806875-a9bc863f-8827-48de-b4de-3b12937c3b11.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165807447-3e55e677-5960-445c-9b65-d08c51d48784.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165807978-00742930-aa0d-485c-8f84-51ff76ce3bd2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165808574-d6e8b5a6-3dd0-42a3-83aa-2da0252d5b0c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/165809913-1438dd9b-bb23-4e72-85b7-d1890eca4303.png)\n",
    "\n",
    "Multinomial Naive Bayes is used when the input data features consists of numerical discrete values (like counts of words, TF-IDF scores, etc) Bernoulli Naive Bayes is used when the input data features consist of only boolean/binary values.\n",
    "\n",
    "Let us assume our dataset consists of continuous, binary and disrete numerical data. Then, \n",
    "\n",
    "- a) For continuous numerical features, we have to compute the likelihoods using the PDF formula\n",
    "- b) For discrete and binary data, we can compute the likelihoods by taking number of occurrences into consideration.\n",
    "\n",
    "When it comes to class label prediction, you have to manually compute the values of $P(y_q=1|x_q)$ and $P(y_q=0|x_q)$.\n",
    "\n",
    "If the given data is in numerical continuous format, then we have to go for GaussianNB.\n",
    "\n",
    "If the given data is present in binary format, then we have to go for BernoulliNB.\n",
    "\n",
    "If the given data is present in discrete numerical format, then we have to go for MultinomialNB.\n",
    "\n",
    "The above mentioned variations of Naive bayes, give the best results on the type of data mentioned above. Our main intention is also to make the model give the better results in prediction.\n",
    "\n",
    "Link: http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e98fef",
   "metadata": {},
   "source": [
    "__Imbalanced Data__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166093639-457a0ced-0a9f-49b0-9a4b-0960ae1afd23.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166093698-8304e3cc-3ee3-4271-bc65-b6b304205c19.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166093774-45caa2b9-40c7-4bbb-bbb4-1e814f229b9c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166093868-10ccdf99-62aa-485c-8163-6de1082fddc7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166093962-c132799f-4e05-4bbf-b3f9-07f82abd0c1f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166094118-db4b4c6a-3189-4f7a-bd50-0360bd8283aa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdf7ce",
   "metadata": {},
   "source": [
    "__Outliers__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166094285-fc339184-5a45-4cd3-a2bd-d17144c76070.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166094334-1a23d976-853d-472c-8d5e-409e06583713.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166094414-73aaa949-3c26-46e5-83d4-9250911b3648.png)\n",
    "\n",
    "\n",
    "1. We will remove the outliers in the training data.\n",
    "2. If you are talking in terms of text, we can treat outliers as rare words. We can create a certain number of vocab which are less than some frequency and make them outliers. And we can remove these words in the train and test data.\n",
    "3. Laplace smoothing is another way to deal with rare words other than removing those words, we will try to give some less probability to rare words and out of vocab words. So there is no need to deal with the certain number of vocab.\n",
    "4. Either you remove words before creating the total probability values or add the Laplace smoothing in the train as well as test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc66395",
   "metadata": {},
   "source": [
    "__Missing Values__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166094605-03af52fc-4b92-405e-9bce-0ea05b7e5fbb.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166094636-b8f0f004-5b92-4402-befb-3dc98bdaa334.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662ec86",
   "metadata": {},
   "source": [
    "__Handling Numerical Features (Gaussian NB)__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166094906-85b196d3-f601-476e-8b25-d6411aa4c5e8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166095270-98e7053e-8707-479c-abd3-837dc5a67017.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166095215-51408c0c-9f54-4ffa-ade8-92c320f4b9d8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166095455-6afd62b8-a03f-472e-9d2c-6ab934f2b22e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166095496-d212be2b-db9a-4a45-847a-9cb61976d904.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166096145-69cb4ce0-774f-40e1-8272-6f704d0842dd.png)\n",
    "\n",
    "Link: https://youtu.be/pYxNSUDSFH4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c75cf4",
   "metadata": {},
   "source": [
    "__Multiclass Classification__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166096263-c0f03b6b-795f-4ac9-bb01-0f3b0356cff1.png)\n",
    "\n",
    "Multinomial Naive Bayes is a specialized version of Naive Bayes that is designed more for text documents. Whereas simple naive bayes would model a document as the presence and absence of particular words, multinomial naive bayes explicitly models the word counts and adjusts the underlying calculations to deal with in.\n",
    "\n",
    "If the input data is in binary format or if we want to build a naive bayes model using binary form of data matrix, then we have to go for Bernoulli NB.\n",
    "\n",
    "If the input data is numerical and discrete(not continuous) data matrix and if we want to build a naive bayes model, then we have to use Multinomial NB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61291446",
   "metadata": {},
   "source": [
    "__Similarity or Distance Matrix__\n",
    "\n",
    "NB is not distance based method, but instead, it is probability based method.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166096763-5d3539c2-326e-42e2-a64e-e62450c9a2a9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ffc30",
   "metadata": {},
   "source": [
    "__Large Dimensionality__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166097165-c8f81ca2-9b95-4d6a-9f6c-0231391b6cf7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61623b71",
   "metadata": {},
   "source": [
    "__Best and Worst Cases__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166097319-5a0e2fab-c437-41a9-99d0-b1f24672e548.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166097385-0f7ae5a3-a4d2-41f0-99d3-9eeab9beeb5d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166097485-d8af3075-3de3-4e82-8227-7edbdc2f680c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/166097535-289545b6-228b-4294-b049-167b52f82749.png)\n",
    "\n",
    "The most fundamental and important assumption in this algorithm is the conditional independence of features. As the features become conditionally dependent on one another the more the dependence the more the performance of this algorithm reduces.\n",
    "\n",
    "Although theoretically speaking the features have to be strictly conditionally independent but practically it has been shown through research that some level of dependence does not affect this algorithm and it works reasonably well. As an example when we are given a positive review is a highly likely that words like good, great and phenomenal occur in this positive review. Now we know that logically speaking these words are sort of dependent on one another which means that given one of these word is present there's a high chance that the other would also be present. But despite of such dependence the model works well which means it can handle some level of conditional dependence among features.\n",
    "\n",
    "Naive bayes algorithm works very well in text classification tasks like email spam filtering and review polarity detection and is considered as a baseline or benchmark in these tasks. Also it works very well for categorical features but in case of real valued features it is seldom used when the distribution is not gaussian especially in the domain of Internet applications where most features do not follow Gaussian distribution.\n",
    "\n",
    "Naive bayes algorithm is easy to interpret and the important features can be found from the model itself. It has low run time and training time complexity and also low runtime space consumption\n",
    "\n",
    "The one disadvantage of this algorithm is that it is easy to overfit the data if we do not do laplace smoothing by finding an appropriate value of alpha by using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450763a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
