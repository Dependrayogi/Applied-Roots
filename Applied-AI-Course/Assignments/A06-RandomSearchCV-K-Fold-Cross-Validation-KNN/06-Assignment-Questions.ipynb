{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963084c5",
   "metadata": {},
   "source": [
    "__1. Why do we use cross validation set in our models?__\n",
    "\n",
    "We use cross validation set to find the optimal hyper-parameters which can generalize the unseen test data. If we do not use cross validation set and only divide the data $D_n$ into train set and test set, then with this approach there is a high possibility that our model under performs the unseen data in the future. Hence using cross validation set becomes imperative to find the best hyper-paramerters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3e0a35",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803245d6",
   "metadata": {},
   "source": [
    "__2. For k fold cross validation, which of the following is/are true and give reasoning behind why you think its True or False?__\n",
    "1. the set of points in each folds are mutually exclusive\n",
    "2. cross validation prevents model from overfitting\n",
    "3. we use stratified k-fold cross validation in skewed datasets for preserving the percentage of samples for each class\n",
    "4. The default number of folds in sklearn.model_selection.KFold is 3\n",
    "\n",
    "__Option 1 is True.__ Reason: We divide the data $D_n$ into mutiple folds (one of the folds is a cross validation set). Union of folds leads to $D_n$ and intersection of the folds leads to $\\phi$. Hence, option 1 is True.\n",
    "\n",
    "__Option 2 is True.__ Reason: Cross validation helps the model in generalizing the unseen data which in turn prevents overfitting.\n",
    "\n",
    "__Option 3 is True.__ Reason: Stratified k-fold is better compared to normal k-fold. Normal k-fold uses random sampling and stratified k-fold uses stratified sampling. Stratified sampling ensures the percentage of samples are preserved for each class.\n",
    "\n",
    "__Option 4 is False.__ Reason: As per the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html), the default number of folds is 5, in earlier version it was 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe686a18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59660db1",
   "metadata": {},
   "source": [
    "__3. What is data leakage?__\n",
    "\n",
    "When we split the data $D_n$ into train and test sets, it is our responsibility to ensure no data is shared between these two sets. When we are training the model, the model should not have any idea/clue about the test set. Test set should be totally new and unseen. But if the data is shared between thse two sets, meaning, model knows what is in the train set and test set, then this situation is called data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182767b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e266d",
   "metadata": {},
   "source": [
    "__4. Which train-test based splitting should be used for time series data and why?__\n",
    "\n",
    "For time series data, the best way is to use time based splitting (TBS) technique to split the data $D_n$. As the time progresses, new data points will be added in $D_n$ and this can be used as test set.\n",
    "\n",
    "Random splitting makes no sense for time series data, because it can break things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c088bdad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b4967",
   "metadata": {},
   "source": [
    "__5. What are different cross validation techniques used for regression problems?__\n",
    "\n",
    "1. Holdout cross validation\n",
    "2. K-fold cross validation\n",
    "3. Stratified K-fold cross validation\n",
    "4. Leave-p-out cross validation\n",
    "5. Leave-one-out cross validation\n",
    "6. Rolling cross validation\n",
    "7. Monte Carlo cross validation\n",
    "\n",
    "Source: [here](https://www.analyticssteps.com/blogs/7-types-cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbb70a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7cf6bc",
   "metadata": {},
   "source": [
    "__6. How training and CV scores help you to find an optimum hyper-parameter for your model?__\n",
    "\n",
    "After splitting the data $D_n$ into train, cross validation and test sets, we fit the model. Fitting the data means to find the patterns in the data.\n",
    "\n",
    "In training phase we find the training accuracy and the training error and in cross validation phase we find the validation accuracy and validation error. We also take a closer look if the model is overfitting or underfitting or well-fitting.\n",
    "\n",
    "The hyper-parameter which has more validation accuracy and less validation error will be the optimum hyper-parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8d1e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec3af2",
   "metadata": {},
   "source": [
    "End of the file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
