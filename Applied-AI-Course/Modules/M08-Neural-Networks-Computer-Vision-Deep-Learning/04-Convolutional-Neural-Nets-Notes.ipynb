{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85e0cc6",
   "metadata": {},
   "source": [
    "__Biological Inspiration: Visual Cortex__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50d0cf",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/194917696-e81f610c-bd55-4173-9252-d5c5ad5579fe.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194918516-ecca5408-02ae-4b23-a289-d9d83a62948e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194919153-41b147db-2cd9-4add-9f3d-8a5ce6f860f9.png)\n",
    "\n",
    "Link: https://youtu.be/v20-E_2bT2c\n",
    "\n",
    "![](https://www.frontiersin.org/files/Articles/34845/fpsyg-04-00124-HTML/image_m/fpsyg-04-00124-g001.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d797f96",
   "metadata": {},
   "source": [
    "__Convolution: Edge Detection on Images__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3b392",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/194922659-f849a1c9-fe75-4103-a221-3615a7efb339.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194922459-dfc65265-f83e-48d6-b070-739788aad07c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194923268-7d58deed-652c-4ac4-82ab-86796194329e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194923689-51bc4680-be56-457f-b5fc-b0b857405668.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194924528-4b16e9fd-565a-4087-9876-5b9348ef87c9.png)\n",
    "\n",
    "Instead of taking zero pixel value as white, it is customary to take zero as black and 255 as white.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194925109-86fa400b-e495-4dae-9539-7edc4b6024a8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194925303-734da18a-a021-413b-abac-2f872a9c2022.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194926570-7bb30603-b782-46a0-8d5f-30d34ae39792.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e55bb",
   "metadata": {},
   "source": [
    "__Convolution: Padding and Strides__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8ab58",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/194927484-091f0106-75ea-4dc9-8e95-b04a12b38ed8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194927800-67714ea4-f1c3-4c08-b4f6-123068b065a8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194928076-a839f487-94b3-4701-9f87-7ef2192e45e2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194928489-887852e0-9880-4d1f-8cc7-53b8c7e6a263.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194928760-a5920e21-d357-4804-8339-14d7c8ecc051.png)\n",
    "\n",
    "Some reasons why padding is important:\n",
    "\n",
    "1. It's easier to design networks if we preserve the height and width and don't have to worry too much about tensor dimensions when going from one layer to another because dimensions will just \"work\".\n",
    "2. It allows us to design deeper networks. Without padding, reduction in volume size would reduce too quickly. (with increasing the stride reduction in volume is much more apparent)\n",
    "3. Padding actually improves performance by keeping information at the borders (it means that without padding the spatial size slowly reduces after each conv and we lose border information,and padding stops this).\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194929927-208cb5dc-aaf0-488d-b2ff-68b3a31ba520.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267ecda",
   "metadata": {},
   "source": [
    "__Convolution over RGB Images__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11bb74",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/Beyoglu_4671_tricolor.png/1280px-Beyoglu_4671_tricolor.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194971734-9958e273-6290-4401-b557-4e8d98a2e8f7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194971793-9a55e554-f496-4142-8460-5a80ec18c6c7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194972163-c0ccfaeb-7934-41dd-83c4-4693b045c3c0.png)\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*QsxXZN0Pq9ZL_lckPtW6pw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a45ff",
   "metadata": {},
   "source": [
    "__Convolutional Layer__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeddba1a",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/194973537-2c73bef7-7736-41c7-af3b-75c1448a116f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194973767-ac7a15e2-fc97-4f65-8648-aa131d89c4bb.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194974002-72a488c8-f39e-4bc4-a3c5-98de8729647d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194974311-be2b9d37-18d1-4075-8acc-5df6a48e3c1a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194974504-f1bfe217-01f0-4c01-b6dc-c716eb909aa0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194974651-6fc3720f-d377-4528-9700-2132bc8b3bd2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194974841-92dc18af-903c-4929-8212-8a7d8389c874.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194975266-94effdc2-5f1a-4fda-b36d-277f8b1dd626.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194975875-66b1c9d5-b929-4313-b8dc-142d5e88642b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/194975946-15903c4c-6877-4bfb-acee-408f51658e67.png)\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*z5wfLwBCEgAPTI_U1mSaVA.png)\n",
    "\n",
    "Link: https://cs231n.github.io/convolutional-networks/#pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a4768",
   "metadata": {},
   "source": [
    "__Max-Pooling__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956f6fe",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/195150219-1a7767a5-27c6-4c75-84b5-7960deff90cc.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195150859-d16c229a-b300-4635-8f01-a2a59f1afb34.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195151477-dccd69d7-fab1-4066-ac9c-1f2c57303d02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d9231",
   "metadata": {},
   "source": [
    "__CNN Training: Optimization__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1352cc",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/195156641-57ce3ccf-c18c-44d8-b33e-a6f5f5b661e3.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195157174-ef3ace4b-2ffc-4860-85de-00fc1f03114c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195157702-6e67be03-68bc-42c7-9c1c-a15e790d6f70.png)\n",
    "\n",
    "![](https://qph.cf2.quoracdn.net/main-qimg-a3dc67927978e056ceac32d5a587ddd8-pjlq)\n",
    "\n",
    "Link: https://poloclub.github.io/cnn-explainer/#article-convolution\n",
    "\n",
    "Link: https://neodelphis.github.io/convnet/maths/python/english/2019/07/10/convnet-bp-en.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2f47f",
   "metadata": {},
   "source": [
    "__Receptive Fields and Effective Receptive Fields__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc053e",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/195406103-32fad38b-9461-4124-a25d-ab2f99c1be66.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195406391-62491aea-3b37-4f32-82cb-cc7bf5562393.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38727550",
   "metadata": {},
   "source": [
    "__Data Augmentation__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c7fe0",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/195412852-18f38d2b-e3e2-4c63-af75-6c9cad7077a2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195413367-1ab1f3e0-2550-468b-8b1b-45b1ca10ae74.png)\n",
    "\n",
    "![](https://static.javatpoint.com/tutorial/machine-learning/images/bias-and-variance-in-machine-learning.png)\n",
    "\n",
    "Underfitting happens when model is too simple, if you have choosen the complex algorithm which learns the data so well and if dataset is small, the model will overfit.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195415405-2f61203f-b5b8-455f-85c2-2b26b44c5aba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f67855",
   "metadata": {},
   "source": [
    "__AlexNet__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3ce98",
   "metadata": {},
   "source": [
    "![](https://i0.wp.com/ramok.tech/wp-content/uploads/2017/12/2017-12-31_01h31_40.jpg)\n",
    "\n",
    "Link: http://euler.stat.yale.edu/~tba3/stat665/lectures/lec18/notebook18.html\n",
    "\n",
    "Paper: https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a76eac",
   "metadata": {},
   "source": [
    "__VGGNet__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14de6f",
   "metadata": {},
   "source": [
    "Link: https://www.quora.com/What-is-the-VGG-neural-network\n",
    "\n",
    "Link: https://github.com/fchollet/deep-learning-models/blob/master/vgg16.py\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1409.1556.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c06f8",
   "metadata": {},
   "source": [
    "__Residual Network__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ba98b",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/195498473-898e35ed-0d95-48e4-b916-3cadbfb88b00.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195496547-73b40a14-aef9-49f0-a04e-6beaaf8721bd.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195496938-e2b3e5c5-32bc-4d61-81e2-d9ed6d927049.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195497197-0ab67571-e949-4b5a-ad98-66115a82d01b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195499005-0a10aee0-3bff-4049-b618-47016d331d39.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195499701-2dba8133-2ac6-4848-bf78-7e1579e5ae9d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195500663-be5b8d0d-ea4c-4098-819d-c23783450352.png)\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da279b0",
   "metadata": {},
   "source": [
    "__Inception Network__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bda215",
   "metadata": {},
   "source": [
    "Link: https://blog.jovian.ai/understanding-the-architecture-of-the-inception-network-and-applying-it-to-a-real-world-dataset-169874795540\n",
    "\n",
    "Link: https://github.com/keras-team/keras-applications/blob/master/keras_applications/inception_v3.py\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1512.00567.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9f1b9",
   "metadata": {},
   "source": [
    "__Transfer Learning__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d35ddd",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/63338657/195654955-d348a301-916a-4f16-ac7a-a42e4ec8b7de.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195656029-9104625e-4939-44a5-9eb7-c14bf089bd15.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195657376-bf8a9d06-6a45-4904-a68f-7bccf6631b2e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195657792-d36de9b6-3ded-4f27-b57e-abee282e756d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195658322-57d89af5-df8a-41de-9753-cc92c2c30b21.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195658862-8cbd4e12-881e-4790-9dcb-721cf8ce77ae.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195659600-973b8f6e-4ac3-47ba-b2b6-a5710845b09b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195660032-5f7e87f7-3246-474c-91e5-1534bf76af86.png)\n",
    "\n",
    "Link: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "Link: https://cs231n.github.io/transfer-learning/\n",
    "\n",
    "Link: https://pechyonkin.me/capsules-1/\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/195970876-777d32af-f5a8-4da0-a73a-a413465e80e1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513f423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
