{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d13f555",
   "metadata": {},
   "source": [
    "This was one of the hardest concepts to digest I guess, because of some missing connections maybe. Anyway, the picture is quite clear except for few questions. Edit: after going through Jay Alammar's blog, the concept has become crystal clear, I guess. So, I’d try to answer my own doubts, please review them.\n",
    "\n",
    "1) At 19:36, you say it's called self-attention because it's paying attention on the words of its own sentence. What is the purpose of self-attention? How does it help with the case of translation? Ultimately our purpose is to find out the attention required by the translated word with respect to the input words, like in attention mechanism. My context vector would itself compute the attention required for each of the translated word with respect to my input words using back propagation. In short, I can't digest why Self Attention>Normal Attention.\n",
    "\n",
    "Answer - The Transformer model takes care of both. It first computes self-attention, i.e. sees how much a word is affected by other words, edits its embeddings in a way to carry this information, and at the output of last encoder layer, it gives embeddings with self-attention considered. Now when this is passed to encoder-decoder attention layer in decoder block, it computes the attention of output words with each of these input words. So, we get the best of both worlds, I guess.\n",
    "\n",
    "2) Why are we not using the powerful concept of RNN. I mean how does is perform better than Encoder-Decoder Attention Model discussed earlier. What if I add several encoder and decoder layers in my attention model, and also add residual connection, and let my last encoder layer go to the decoder layer along with its attention weights (or context vector in short). Why the non-RNN model is performing better than an RNN model. Ok, I agree that it is computationally faster than RNN, but what about performance?\n",
    "\n",
    "Answer - This is because in Transformers, we include the powerful concept of self-attention along with attention. Along with this, if we were to do such a multi-layered RNN computation, that would be highly expensive. Using the transformers, we can trivially parallelize our training during passage through the FFNN layers through which each word's embedding is passed independently parallelly. Hence, it is easier to build multi layered Encoder - Decoder Transformer. (Please bear with me if I've used the word transformer wrongly here, just talking about this whole model.)\n",
    "\n",
    "3) At 30:30. We are summing up the SoftMax v1, SoftMax v2 SoftMax v3.. and so on. What good does addition do.\n",
    "\n",
    "Answer - It is like weighted average of each word's embedding right. Which captures how much does the word depend on another word.\n",
    "\n",
    "4) Why use multiheaded Attention –\n",
    "\n",
    "Answer - Intuitively each head would find the new vectors with some dependencies from other words, but the word would be highly dependent on itself (naturally) so a single head wouldn't be able to capture all the contexts with other words, that's why multi heads.\n",
    "\n",
    "5) I think that there hasn't been much of discussion as to how the decoder would work, so here is my summary and takeaway from both the blog and the video.\n",
    "\n",
    "Answer - The output from the last FFNN layer of encoders would be taken and converted into matrices of Keys and Queries. Their shape would be (say we have 5 words and 64 dimensional embeddings) - (5x64) for single head. Now in the decoder, we send word first and let it generate output. Now we add the generated output to input again i.e. now input is o1, next o1 o2, etc.\n",
    "These are also embedded first and then added to positional embeddings. Now they pass through self-attention layer where their embeddings are adjusted so as to capture the dependencies among themselves. Next it to add and normalize (skipping this, as its fairly trivial to understand). Next is the important Encoder - Decoder Attention. Here the Keys and values from the encoder output are used, and the Queries are created from the outputs of decoder self-attention (as per the blog). So now again we calculate the (Query x Key_T) value and then SoftMax, then add all the (SoftMax * Values) for each word. This way we get attentions from both decoder inputs themselves and their attention with the original sentence too. Next, they are passed through FFNN and subsequent layers.\n",
    "\n",
    "One doubt (cleared by myself) in above Encoder-Decoder Attention layer:\n",
    "The Key and Value matrices (from encoder output) are of shape (5x64) for one head. Say my Decoder input length was - 3 words and 64 dim embedding (after self-attention layer). So, Query matrix of decoder is of shape (3x64).\n",
    "Now to compute the Z matrix, i.e. embeddings with self-attention of these decoder words –\n",
    "Query x Key_T which will be (3x64) x (64x5) – outputs matrix of shape (3x5) and now we multiply it with value matrix which is of shape (5x64) – so final Z matrix is of shape (3x5) x (5x64) = (3x64) – so we get embeddings with self-attention and attention with input for each of the decoder input word, of 64 dims (latent).\n",
    "\n",
    "One doubt not cleared is:\n",
    "In the figure at 38:04, has the word “it”’s dependency with itself been excluded and normalized accordingly so as to find the relation with others?\n",
    "\n",
    "Hope the questions answered by me are correct. Thank You.\n",
    "\n",
    "---\n",
    "\n",
    "Good explanations. It’s dependency with itself been excluded and normalized accordingly so as to find the relation with others? -  Self-attention may get the attention vector which gives the attention vectors and tells how each word is dependent on attention finding a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24c582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
