{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07de70c8",
   "metadata": {},
   "source": [
    "__Geometric Intution__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170386498-99b3361e-0fa7-488c-b4a9-cc10f367412a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170386845-6e852672-6041-491e-b177-fdd13779df10.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170387153-207df5f3-6fd2-44f4-ba98-3a5b4151f744.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170387324-c5da4596-0232-4728-9cfc-31edaa410197.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170387661-30ba2631-b043-4c76-a4d4-250391d33f38.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170388062-97da510d-aa1f-4f96-b31b-dc47e0225420.png)\n",
    "\n",
    "SVM minimizes hinge-loss while Logistic Regression minimizes logistic-loss.\n",
    "\n",
    "The objective of margin maximization is to choose a hyperplane which separates positive and negative points as far as possible.\n",
    "\n",
    "Link: https://web.archive.org/web/20190501131948/https:/towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f?gi=4eeeb78570fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304e3df",
   "metadata": {},
   "source": [
    "__Mathematical Derivation__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170811316-9765c9d5-0009-4585-ad43-d0fed4a5486c.png)\n",
    "\n",
    "The normal vector $w$ need not be unit vector.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170811427-4b19b74c-56e8-410d-a4ec-a66c1fbfb548.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170811472-41ae4d69-aa6b-41d8-ae51-0f91cb2ecdb0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170811670-997913cd-97c9-4d2e-9506-88a4d05172f7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170811874-a6d50b0a-5639-4fb1-8262-ee8eaf98e423.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170811927-e8581af7-1bcd-4885-9510-b91ebd39f8c2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170811980-9891483c-9da8-445b-87dc-dc6ff70e5911.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170812102-02c4ce7e-8739-4477-8d0c-0839c8659ad6.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170812344-3167f736-e39c-4b19-9f26-80c07fd21d86.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170812405-60517ce1-fc8a-40b9-a257-baf488e20537.png)\n",
    "\n",
    "\"C\" only controls the tradeoff between hinge-loss minimization and regularization SVM. It is exactly like lambda in LogisticRegression that we saw earlier.\n",
    "\n",
    "In SVM formulation, the hinge-loss is the zeta_i's and the regularization is same as margin-maximization. Our goal is to minimize hinge-loss while also having a good amount of regularization or margin. It is not just one of them. It is a combination of both and \"C\" controls how much importance we want to give to both of them. \"C\" is to be determined by hyper-param tuning.\n",
    "\n",
    "Large Value of parameter C => small margin\n",
    "\n",
    "Small Value of paramerter C => Large margin\n",
    "\n",
    "![](https://miro.medium.com/max/1182/0*z00-0ici9ikQLBug.jpg)\n",
    "\n",
    "As the value of C increases the loss term also increases very largely. So argmin will concentrate more on decreasing the loss value. So small increases in the regularisation term will go unnoticed by argmin as there is a huge increase in the loss term. This small increase in the regularisation term results in the reduction of margin, because this regularisation term is the reciprocal of margin(as x increases 1/x decreases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0920b",
   "metadata": {},
   "source": [
    "__Why we take values +1 and and -1 for Support vector planes?__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170815142-682034a2-be39-4a06-b9ec-4fea30e8877d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170815186-04e5689a-8a89-4fe1-9dd0-87ea4a823397.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170815279-78a07f46-ab44-4b71-a941-b951fef5dc07.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170815353-cd8b015f-6aca-4ece-be39-aa0d8b46f2e4.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170815429-d98766b5-2016-4c34-b474-16a2133c88d7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2152837",
   "metadata": {},
   "source": [
    "__Loss Function (Hinge Loss) based Interpretation__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170831780-b7025dac-711a-4300-9cda-c9b772bae00a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170831994-a28bf73f-3f54-44b4-a9ac-7776f384e857.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170832282-1213cfaf-cec4-4019-9973-f13b2aba9a48.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170833001-8def12d7-15b0-4b1a-a211-5d39efd055a7.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170832450-06f4f241-c5d2-481d-a92d-7687f840b12a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170832886-6f012478-e14a-4138-a568-7cdf18363e3d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170832946-63db4689-190b-44ae-8031-93fa17e9ef4a.png)\n",
    "\n",
    "Link: https://youtu.be/8xbnLHn4jjQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a2553",
   "metadata": {},
   "source": [
    "__Dual Form of SVM Formulation__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170846168-b4b278b9-c4bd-4c25-a0e7-96097df4baad.png)\n",
    "\n",
    "Correction 1: The dual formulation should contain $y_iy_j$ and not $y_iy_i$.\n",
    "\n",
    "Correction 2: In the dual formulation, instead of having the constraint $\\alpha_i \\ge 0$, the correct constraint is $C \\ge \\alpha_i \\ge 0$.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170846283-bfb6947b-5ee7-4179-8edc-2ee762f402a6.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170846378-4d429565-f2cd-4226-bb7d-652bf5992a61.png)\n",
    "\n",
    "Correction 1: The dual formulation should contain $y_iy_j$ and not $y_iy_i$.\n",
    "\n",
    "Correction 2: In the dual formulation, instead of having the constraint $\\alpha_i \\ge 0$, the correct constraint is $C \\ge \\alpha_i \\ge 0$.\n",
    "\n",
    "Correction 3: In the dual formulation, it should be $\\alpha_i > 0$ for SV’s and $\\alpha_i = 0$ for Non-SV’s.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170846449-bbf91645-375b-46c0-96ee-3899c9795f5e.png)\n",
    "\n",
    "Correction 1: In the dual formulation, instead of having the constraint $\\alpha_i \\ge 0$, the correct constraint is $C \\ge \\alpha_i \\ge 0$.\n",
    "\n",
    "Correction 2: In the dual formulation, it should be $\\alpha_i > 0$ for SV’s and $\\alpha_i = 0$ for Non-SV’s.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170846519-785dab5e-4559-459e-af74-5ec177c4774b.png)\n",
    "\n",
    "Geometric interpretation of Lagrange's multipliers is: Given an optimization problem under some constraints, Lagrange multipliers represent how an optimization function changes w.r.t that constraint. Roughly, lambda = (Partial differentiation of function to be optimized) / (Partial differentiation of constraint).\n",
    "\n",
    "Link: https://stats.stackexchange.com/questions/19181/why-bother-with-the-dual-problem-when-fitting-svm\n",
    "\n",
    "The $\\alpha_i$'s are called as Lagrange multipliers. Initially, in its primal form, we have the linear SVM equation with a constraint. To remove this constraint: https://i.imgur.com/NaO4Fa8.png (marked in red) and bring it into the optimization we introduce the Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cd224",
   "metadata": {},
   "source": [
    "__Kernel Trick__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854135-1c2849f4-3ee9-4f6a-947f-fb616ed8aa9c.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854157-2e4c74b2-56d2-4e28-84e1-e8013cde9694.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854223-a50a0c9f-3cd1-4fea-8007-c705139bdf36.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854279-db34385b-9df8-4bdf-9135-5fa33566b4f8.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854315-8edc7208-119c-4761-9bb5-14eb1a80c81f.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854527-50c14d86-40e3-427f-b148-ad96d497400b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f8fb9",
   "metadata": {},
   "source": [
    "__Polynomial Kernel__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854626-cd872ebe-44f3-42c3-aac3-964c720d7d88.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854849-30961420-9da3-4b42-b522-dc09d036fb3b.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854895-45aa4da1-2690-439e-9607-2caaed4da14e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170854981-80b2c253-a0e6-4e3b-a91e-fa9a84d66cb6.png)\n",
    "\n",
    "Link: https://youtu.be/itlOae61Oak\n",
    "\n",
    "Link: https://soundcloud.com/applied-ai-course/kernels-vs-feature-transforms/s-HesVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0d78d",
   "metadata": {},
   "source": [
    "__RBF Kernel__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170859900-1d35d7e8-1fc5-4ee8-9af7-38a42bec9c66.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170859989-f02f3627-10f3-4cde-b75e-4caedecf2333.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860154-4270c4ae-6239-408a-9c19-30ca6a8db1ce.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860213-8fb800b7-b7cb-4dd8-8ccf-3c290f6cc18d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860288-eb264503-038d-4f2e-9228-fc692b703a10.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860360-730cd1fa-c66c-4957-861b-9e124b955e38.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860543-d3e0b3a6-be89-48e7-8a7d-f1e611c390d3.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860619-0d1dac17-cb61-4251-a4e5-ae0f1716c992.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860678-20d16b6a-063e-40d4-a917-cfc856b20878.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170860730-bcabfb62-b5d1-4518-b4cf-a1b44118420d.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170861229-96045f06-49b6-47e4-8a48-181729b9b97e.png)\n",
    "\n",
    "Link: https://soundcloud.com/applied-ai-course/rbf-svm-vs-knn\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170861344-72c94e7c-0710-4a6d-bb16-cf7a250627db.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170861413-9b934fe1-3f6a-4ed8-a4fd-8a60207fb76c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706eafb2",
   "metadata": {},
   "source": [
    "__Domain Specific Kernels__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170865941-800a95b8-ed9f-44ba-8c72-516bb92268e6.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170865991-af86ead0-ae61-455f-9b4e-52a59a7be8ef.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170866046-9d640f4e-fbbe-4d8f-857d-d2cefe8677cd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccb295",
   "metadata": {},
   "source": [
    "__Train and Runtime Complexities__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170866357-88efee3c-ff19-42f8-94d7-9de132c73ae0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170866458-1b341777-2fd6-4f64-bf9e-088e32676f36.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170866570-a26df105-3783-4402-8138-282ff2e6d8d9.png)\n",
    "\n",
    "Link: https://soundcloud.com/applied-ai-course/which-model-to-use/s-ohyu8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa13505",
   "metadata": {},
   "source": [
    "__nu-SVM: Control Errors and Support Vectors__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170867424-0e909500-384a-4f35-8924-bc14cc587229.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170867505-391f1a88-c4d5-4e65-95dc-b5f03179626c.png)\n",
    "\n",
    "nu SVM seems to be computationally more expensive and harder to optimize for but it offers an advantage in the form of interpretability: The parameter nu is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors relative to the total number of training examples. For example, if you set it to 0.05 you are guaranteed to find at most 5% of your training examples being misclassified (at the cost of a small margin, though) and at least 5% of your training examples being support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8974b",
   "metadata": {},
   "source": [
    "__SVM Regression__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170867893-68ac4c62-efc8-4559-b7ef-f8f5f4018a29.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170867947-f7b99ffc-a65f-49a5-b152-095f0432cba2.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170868275-4d96d2fb-057b-4def-b4a1-8c9920e7b4d9.png)\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Svr_epsilons_demo.svg/1920px-Svr_epsilons_demo.svg.png)\n",
    "\n",
    "Support-vector regression (prediction) with different thresholds ε. As ε increases, the prediction becomes less sensitive to errors.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170868390-2a27f98e-e927-4273-9844-820235ada54b.png)\n",
    "\n",
    "Link: https://youtu.be/kgu53eRFERc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262b0de",
   "metadata": {},
   "source": [
    "__Cases__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170870506-ac9eae9c-9feb-462a-b927-5a6428209710.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170870673-e0a8462c-d253-4a58-ab98-639197a57416.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/170870837-303e6550-3bb7-4a75-b650-d49bdfc2e4ee.png)\n",
    "\n",
    "Link: https://soundcloud.com/applied-ai-course/svm-vs-deep-learning-for-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6f396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
