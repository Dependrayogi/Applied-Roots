{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7de7d09",
   "metadata": {},
   "source": [
    "__PCA (Principal Component Analysis)__\n",
    "\n",
    "Link: https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n",
    "\n",
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159118058-f52b161a-ecee-43f3-bbfe-0f12993d6cf3.png)\n",
    "\n",
    "What are the application of dimensionality reduction?\n",
    "\n",
    "Typically it is used in visualization and also in some cases where there is low computational power and we want to deploy the model. In that case we select top important features by doing PCA and then deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a050f7e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001fa3b",
   "metadata": {},
   "source": [
    "__Geometric Intuition of PCA (simple example)__\n",
    "\n",
    "Best resource: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579\n",
    "\n",
    "Link: https://youtu.be/FgakZw6K1QQ\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159118689-3377bd05-b90e-4177-bf73-6d8770929a19.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159118793-516f85f7-a430-42b1-9c0a-3b7800d4e91f.png)\n",
    "\n",
    "Another example\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159118913-1fe9f998-c73d-4946-8da1-dd2138d97893.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159119261-1d52906c-2ce5-4dea-8b9c-6534e637d37e.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159119375-76b7ad2a-8abb-4d6c-92aa-318d447dc38a.png)\n",
    "\n",
    "Note: PCA never eliminates or drop any feature instead it constructs some new features which can explain more about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6b40a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8eda09",
   "metadata": {},
   "source": [
    "__Mathematical Objective Function of PCA__\n",
    "\n",
    "![](https://i.stack.imgur.com/Q7HIP.gif)\n",
    "\n",
    "Please refer notes - Variance Maximization.\n",
    "\n",
    "Please refer notes - Distance Minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd813b3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445317c9",
   "metadata": {},
   "source": [
    "__Eigen Values and Eigen Vectors (PCA): Dimensionality Reduction__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159149853-db2b4091-66b8-4a38-ad1f-a83fef5de52a.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159149982-ccb22761-52b1-4a64-a159-686f9662d69d.png)\n",
    "\n",
    "Every pair eigen vectors are perpendicular to each other which means the dot product is $0$.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159150111-59557f5d-7c4e-43fe-aec1-171a0ed18259.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159150198-9d88cd3d-4ffa-4a41-9e51-fca34f603ac0.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159150301-b668f3e7-5d5f-4b36-89d7-2967a7b7a423.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159150406-352443e9-36c1-463c-a838-5524f389ff68.png)\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159150581-949b4f2d-7b6c-4700-8ea6-31949f622f6e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565da8b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2cffe",
   "metadata": {},
   "source": [
    "__PCA for Dimensionality Reduction and Visualization__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159151032-c283222a-2b6e-40ba-92b5-6e16ea5f21f6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba153cbf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce79c4e",
   "metadata": {},
   "source": [
    "__Summary__\n",
    "\n",
    "- Principal Component Analysis (PCA) is a statistical techniques used to reduce the dimensionality of the data (reduce the number of features in the dataset) by selecting the most important features that capture maximum information about the dataset.\n",
    "- The features are selected on the basis of variance that they cause in the output. Original features of the dataset are converted to the Principal Components which are the linear combinations of the existing features. The feature that causes highest variance is the first Principal Component. The feature that is responsible for second highest variance is considered the second Principal Component, and so on.\n",
    "- In simple words, Principal Component Analysis is a method of extracting important features (in the form of components) from a large set of features available in a dataset.\n",
    "- PCA finds the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information. By projecting our data into a smaller space, weâ€™re reducing the dimensionality of our feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ef251",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94338d",
   "metadata": {},
   "source": [
    "__Advantages of Principal Component Analysis__\n",
    "\n",
    "- Removes Correlated Features: In a real world scenario, this is very common that you get thousands of features in your dataset. You cannot run your algorithm on all the features as it will reduce the performance of your algorithm and it will not be easy to visualize that many features in any kind of graph. So, you MUST reduce the number of features in your dataset.\n",
    "- You need to find out the correlation among the features (correlated variables). Finding correlation manually in thousands of features is nearly impossible, frustrating and time-consuming. PCA does this for you efficiently.\n",
    "- After implementing the PCA on your dataset, all the Principal Components are independent of one another. There is no correlation among them.\n",
    "- Improves Algorithm Performance: With so many features, the performance of your algorithm will drastically degrade. PCA is a very common way to speed up your Machine Learning algorithm by getting rid of correlated variables which don't contribute in any decision making. The training time of the algorithms reduces significantly with less number of features.\n",
    "- So, if the input dimensions are too high, then using PCA to speed up the algorithm is a reasonable choice.\n",
    "- Improves Visualization: It is very hard to visualize and understand the data in high dimensions. PCA transforms a high dimensional data to low dimensional data (2 dimension) so that it can be visualized easily.\n",
    "- We can use 2D Scree Plot to see which Principal Components result in high variance and have more impact as compared to other Principal Components.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/ac/Screeplotr.png)\n",
    "- Even the simplest IRIS dataset is 4 dimensional which is hard to visualize. We can use PCA to reduce it to 2 dimension for better visualization.\n",
    "- Consider a situation where we have 50 features (p = 50). There can be p(p-1)/2 scatter plots i.e. 1225 plots possible to analyze the variable relationships. It would be a tedious job to perform exploratory analysis on this data. That is why, we have to use PCA to get rid of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5749e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636aa6d",
   "metadata": {},
   "source": [
    "__Disadvantages of Principal Component Analysis__\n",
    "\n",
    "![](https://user-images.githubusercontent.com/63338657/159152437-fdf5d56a-7ccc-43f5-ad32-d3212bea04c8.png)\n",
    "\n",
    "- Independent variables become less interpretable: After implementing PCA on the dataset, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "- Data standardization is must before PCA: You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components.\n",
    "- For instance, if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results.\n",
    "- Also, for standardization, all the categorical features are required to be converted into numerical features before PCA can be applied.\n",
    "- PCA is affected by scale, so you need to scale the features in your data before applying PCA. Use StandardScaler from Scikit Learn to standardize the dataset features onto unit scale (mean = 0 and standard deviation = 1) which is a requirement for the optimal performance of many Machine Learning algorithms.\n",
    "- Information Loss: Although Principal Components try to cover maximum variance among the features in a dataset, if we don't select the number of Principal Components with care, it may miss some information as compared to the original list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea106b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
